name: Database Backup

on:
  schedule:
    # Daily at 09:00 PM IST (15:30 UTC)
    - cron: "30 15 * * *"
  workflow_dispatch:
    inputs:
      reason:
        description: "Why run a manual backup?"
        required: false
        type: string

jobs:
  backup:
    runs-on: ubuntu-latest
    env:
      GDRIVE_FOLDER_ID: 1_6w4x5S9OuSh8inDikd7X3py2202csMj

    steps:
      - name: Install PostgreSQL client v17
        run: |
          set -e
          sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
          wget -qO- https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo tee /etc/apt/trusted.gpg.d/pgdg.asc
          sudo apt-get update -y
          sudo apt-get install -y postgresql-client-17
          echo "/usr/lib/postgresql/17/bin" >> $GITHUB_PATH

      - name: Dump database (explicit v17 binaries)
        env:
          DATABASE_URL: ${{ secrets.SUPABASE_DB_URL }}
        run: |
          set -euo pipefail

          PG_BIN="/usr/lib/postgresql/17/bin"
          PG_DUMP="${PG_BIN}/pg_dump"
          PG_RESTORE="${PG_BIN}/pg_restore"

          echo "=== PostgreSQL client verification ==="
          $PG_DUMP --version
          $PG_RESTORE --version
          which $PG_DUMP || true
          which $PG_RESTORE || true
          echo "PATH=$PATH"
          echo "======================================"

          mkdir -p dumps
          TS=$(date -u +"%Y-%m-%dT%H-%M-%SZ")
          FILE="dumps/backup-${TS}.dump"

          echo "Starting pg_dump..."
          $PG_DUMP "$DATABASE_URL" -Fc \
            --schema=auth \
            --schema=public \
            --schema=storage \
            --schema=graphql_public \
            --schema=extensions \
            -f "$FILE"

          echo "Dump created: $FILE"
          echo "Validating dump contents..."

          # Show first part of archive list for debugging
          $PG_RESTORE --list "$FILE" | sed -n '1,200p'

          # Hard checks for auth tables
          $PG_RESTORE --list "$FILE" | grep -i -E 'auth[[:punct:][:space:]]+users' >/dev/null \
            || { echo "FATAL: auth.users not present in dump"; exit 1; }

          $PG_RESTORE --list "$FILE" | grep -i -E 'auth[[:punct:][:space:]]+identities' >/dev/null \
            || { echo "FATAL: auth.identities not present in dump"; exit 1; }

          gzip -n "$FILE"
          echo "Backup ready: ${FILE}.gz"

      - name: Convert .dump.gz â†’ .backup
        run: |
          set -euo pipefail
          F=$(ls -1t dumps/*.dump.gz | head -n1)
          [ -n "$F" ]
          BASENAME=$(basename "$F" .dump.gz)
          gunzip -c "$F" > "dumps/${BASENAME}.backup"
          echo "Produced dumps/${BASENAME}.backup"

      - name: Install rclone
        run: |
          curl -fsSL https://rclone.org/install.sh | sudo bash

      - name: Setup Rclone Config
        run: |
          set -e
          mkdir -p ~/.config/rclone
          echo "${{ secrets.RCLONE_CONF_BASE64 }}" | base64 -d > ~/.config/rclone/rclone.conf
          ls -l ~/.config/rclone/rclone.conf

      - name: Upload Backup to Google Drive
        env:
          RCLONE_CONFIG_GDRIVE_ROOT_FOLDER_ID: ${{ env.GDRIVE_FOLDER_ID }}
        run: |
          set -euo pipefail
          B=$(ls -1t dumps/*.backup | head -n1)
          [ -n "$B" ]
          rclone copy "$B" gdrive: --drive-chunk-size=64M --transfers=1 -v
          echo "Uploaded $B to Google Drive"

      - name: Cleanup old backups (keep last 3)
        env:
          RCLONE_CONFIG_GDRIVE_ROOT_FOLDER_ID: ${{ env.GDRIVE_FOLDER_ID }}
        run: |
          set -euo pipefail

          FILES=$(rclone lsf gdrive: --files-only | grep '\.backup$' | sort -r || true)
          TOTAL=$(echo "$FILES" | grep -c . || true)

          echo "Found $TOTAL backup(s)"

          KEEP=3
          if [ "$TOTAL" -gt "$KEEP" ]; then
            TO_DELETE=$(echo "$FILES" | tail -n +$((KEEP + 1)))
            for FILE in $TO_DELETE; do
              echo "Deleting: $FILE"
              rclone delete "gdrive:$FILE"
            done
          else
            echo "No cleanup needed"
          fi

      - name: Notify Success
        if: success()
        env:
          DATABASE_URL: ${{ secrets.SUPABASE_DB_URL }}
        run: |
          psql "$DATABASE_URL" -c "INSERT INTO public.system_notifications (type, status, message) VALUES ('backup', 'success', 'Backup completed successfully');" || true

      - name: Notify Failure
        if: failure()
        env:
          DATABASE_URL: ${{ secrets.SUPABASE_DB_URL }}
        run: |
          psql "$DATABASE_URL" -c "INSERT INTO public.system_notifications (type, status, message) VALUES ('backup', 'error', 'Backup failed on GitHub Actions');" || true

      - name: Upload artifact (14-day retention)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: db-backup-${{ github.run_id }}
          path: dumps/*.dump.gz
          retention-days: 14
